# Neural Network Architectures

- ## Basic Architectures
  - [x] MLP 
  - [x] CNN
  - [ ] ResNet
  - [ ] DenseNet
  - [ ] EfficientNet

## Recurrent Networks
- [x] Vanilla RNN
- [x] Bidirectional RNN
- [x] GRU
- [x] LSTM
- [ ] Memory Networks
- [ ] Neural Turing Machines
- [ ] Differentiable Neural Computers
- [ ] Memory-Augmented Neural Networks

## Auto-Encoders
- [ ] Sparse
- [ ] Variational
- [ ] Denoising
- [ ] Contractive
- [ ] Convolutional

## Transformers
- [x] Vanilla
- [x] Linear
- [x] Sparse 
- [x] Gaussian
- [x] Differential
- [x] Universal
- [ ] Negative-Weights
- [ ] FlashAttention (v1/v2/v3)
- [ ] Vision Transformers (ViT)
- [ ] Byte Latent Transformers
- [ ] Transformers^2
- [ ] Performer
- [ ] Reformer
- [ ] Longformer
- [ ] FNet
- [ ] Routing Transformers
- [ ] Perceiver/Perceiver IO

## Diffusion Models
- [ ] U-net
- [ ] U-ViT
- [ ] DiT-based
- [ ] SSM-based

## State Space Models
- [ ] Mamba
- [ ] S4
- [ ] H3

## Physics-Informed Neural Networks (PINNs)
- [ ] Neural ODE
- [ ] Fourier Neural Operator
- [ ] Deep Operator Networks
- [ ] Hamiltonian Neural Networks
- [ ] Lagrangian Neural Networks

## Graph Neural Networks
- [ ] Graph Convolutional Networks (GCN)
- [ ] Graph Attention Networks (GAT)
- [ ] Message Passing Neural Networks
- [ ] Graph Transformers
- [ ] Graph Normalizing Flows

## Generative Adversarial Networks (GANs)
- [ ] DCGAN
- [ ] WGAN
- [ ] CycleGAN
- [ ] StyleGAN
- [ ] BigGAN

## Energy-Based Models
- [ ] Restricted Boltzmann Machines
- [ ] Deep Belief Networks
- [ ] Deep Energy Networks
- [ ] Normalizing Flows

## Specialized Architectures
- [ ] Kolmogorov-Arnold Networks
- [ ] Mixture of Experts
- [ ] Fuzzy Neural Networks
- [ ] Jacobian Fields
- [ ] Spiking Neural Networks
- [ ] Hyperbolic Neural Networks
- [ ] Free-Equivariance Neural Networks
- [ ] Neural Causal Models

# Reinforcement Learning Algorithms

## Value-Based Methods
- [ ] Deep Q-Networks (DQN)
- [ ] Double DQN
- [ ] Dueling DQN
- [ ] Rainbow DQN
- [ ] Categorical 51-Atom DQN (C51)
- [ ] Quantile Regression DQN (QR-DQN)
- [ ] Implicit Quantile Networks (IQN)

## Policy Gradient Methods
- [ ] Asynchronous Advantage Actor-Critic (A3C)
- [ ] Advantage Actor-Critic (A2C)
- [ ] Proximal Policy Optimization (PPO)
- [ ] Trust Region Policy Optimization (TRPO)
- [ ] Deep Deterministic Policy Gradient (DDPG)
- [ ] Twin Delayed DDPG (TD3)
- [ ] Soft Actor-Critic (SAC)
- [ ] Group Related Policy Optimization (GRPO)

## Model-Based Methods
- [ ] World Models
- [ ] Imagination-Augmented Agents (I2A)
- [ ] Model-Based RL with Model-Free Fine-Tuning (MBMF)
- [ ] Model-Based Value Expansion (MVE)
- [ ] Dreamer
- [ ] PlaNet

## Multi-Agent RL
- [ ] Multi-Agent DDPG (MADDPG)
- [ ] Counter-Factual Multi-Agent (COMA)
- [ ] Multi-Agent PPO (MAPPO)
- [ ] Multi-Agent SAC (MASAC)

## Exploration Strategies
- [ ] Hindsight Experience Replay (HER)
- [ ] Random Network Distillation (RND)
- [ ] Never Give Up (NGU)
- [ ] Go-Explore

## Game-Playing and Planning
- [ ] AlphaZero
- [ ] MuZero
- [ ] AlphaGo
- [ ] Monte Carlo Tree Search (MCTS)

## Offline RL
- [ ] Conservative Q-Learning (CQL)
- [ ] Behavior Regularized Actor Critic (BRAC)
- [ ] Implicit Q-Learning (IQL)
- [ ] Decision Transformer

## Meta-RL
- [ ] RL^2
- [ ] MAML for RL
- [ ] PEARL
- [ ] ProMP

## Hierarchical RL
- [ ] Option-Critic
- [ ] Hierarchical Actor-Critic (HAC)
- [ ] HIRO
- [ ] FUN

# Optimization Algorithms

## First-Order Methods
- [ ] Stochastic Gradient Descent (SGD)
- [ ] SGD with Momentum
- [ ] SGD with Nesterov Momentum
- [ ] AdaGrad
- [ ] AdaDelta
- [ ] RMSProp
- [ ] Adam and Variants
- [ ] Adam
- [ ] AdamW
- [ ] NAdam
- [ ] RAdam
- [ ] AdaMomentum
- [ ] AdaBelief
- [ ] AdaFactor

## Large-Scale/Distributed Training
- [ ] LARS (Layer-wise Adaptive Rate Scaling)
- [ ] LAMB (Layer-wise Adaptive Moments for Batch training)
- [ ] Shampoo
- [ ] SOAP
- [ ] FTRL (Follow The Regularized Leader)

## Recent Innovations
- [ ] Lion
- [ ] Prodigy
- [ ] Sophia
- [ ] Muon
- [ ] DeMo
- [ ] Adan
- [ ] Ranger

## Geometric Methods
- [ ] Reimann SGD
- [ ] Natural Gradient Descent
- [ ] Mirror Descent

## Adaptive Learning Rate Methods
- [ ] Cyclical Learning Rates
- [ ] One Cycle Policy
- [ ] Cosine Annealing
- [ ] SGDR (Stochastic Gradient Descent with Restarts)

## Hybrid Methods
- [ ] AdaScale
- [ ] NovoGrad
- [ ] Apollo
- [ ] MADGRAD
- [ ] SAM (Sharpness-Aware Minimization)

## Specialized Optimizers
- [ ] Lookahead Optimizer
- [ ] Rectified Adam
- [ ] AGC (Adaptive Gradient Clipping)
- [ ] LBFGS (Limited-memory BFGS)
- [ ] AdaMax
- [ ] AMSGrad

# Distributed Training
- [ ] MegatronLM
- [ ] GPipe
- [ ] Alpa
- [ ] Tenplex
- [ ] DeepSeed
- [ ] PipeDream
- [ ] ZeRO
- [ ] PyTorch DDP
- [ ] Horovod
- [ ] TensorFlow Distribution Strategy
- [ ] Ray Train
- [ ] FairScale
- [ ] Colossal-AI
- [ ] JAX pmap/pjit
